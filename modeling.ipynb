### Modeling
> The goal of this notebook is to build a model that can predict the status of a Tanzanian well based on the features in our dataset. The model will be evaluated on the accuracy of its predictions. Specifically, if it achieves an accuracy of 75% or higher, it will be considered a success.
To ensure that our models are not overfitting, we will be using the following techniques:
-  Cross Validation
#import libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import plot_confusion_matrix
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.model_selection import cross_validate
from sklearn.tree import DecisionTreeClassifier 
from sklearn.preprocessing import OneHotEncoder
from sklearn import tree
from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
import xgboost as xgb
from xgboost import XGBClassifier
from sklearn.model_selection import GridSearchCV
#preview the new data
import pandas as pd
df = pd.read_csv('cleaned_data.csv')
df.head()
#getting a copy of our dataframe
df1 = df.copy()
#checking the value count of target group
df1['status_group'].value_counts(normalize=True)
The functional group accounts for 54.1% of the dataset. The non-functional group accounts for 38.8% of the dataset. The functional needs repair group accounts for 7% of the dataset.
new_status_group = {'non functional': 0, 'functional': 1, 'functional needs repair' : 2}
df1['status_group'] = df1['status_group'].replace(new_status_group)
df1['status_group'].value_counts()
Since the data type is object, we need to convert it to an integer
df1['status_group'].dtypes
# One hot encode categoricals

categorical = ['source_type','quantity','water_quality','payment_type','management_group','basin']

ohe = pd.get_dummies(df[categorical], prefix=categorical, drop_first=True)
ohe
# Preview the one hot encoded datatypes
ohe.dtypes
# Define X and y
X = ohe
y = df1['status_group']

# Split data to train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

# Print head train
print(X_train.shape)

# Print head test
print(y_train.shape)
We can confirm that the x_train and y_train dataframes have the same number of rows. Therefore, we can proceed to the modelling.
#### 4.1 Logistic Regression
> Logistic regression is a statistical model that in its basic form uses a logistic function to model a binary dependent variable, although many more complex extensions exist. In regression analysis, logistic regression (or logit regression) is estimating the parameters of a logistic model (a form of binary regression).
# Instantiate logistic regression
logreg = LogisticRegression(class_weight = 'balanced', solver = 'lbfgs', random_state=42)

# Build a pipeline with standard scaler and logistic regression
scaled_pipeline = Pipeline([('ss', StandardScaler()), 
                              ('logreg', LogisticRegression())])

# Fit the training data to pipeline
scaled_pipeline.fit(X_train, y_train)

# Predict the labels of the test set: y_pred
y_pred_log = scaled_pipeline.predict(X_test)

# Perform cross-validation: cv_results
cv_results_log = cross_validate(scaled_pipeline, X_test, y_test, cv=3)

# Display cross validation test scores
cv_results_log['test_score']
# Plot the confusion matrix
fig, ax = plt.subplots(figsize=(10,10))

plot_confusion_matrix(scaled_pipeline, X_test, y_test, cmap='Blues', ax=ax)
plt.title("Logistic Regression Confusion Matrix");
The confusion matrix above shows that the model has a bias towards predicting that a well is functional. Furthermore, we see that the model has a higher number of true positives and true negatives than false positives and false negatives.
# Classification report
target_names = ['Non-Functional', 'Functional', 'Needs-Repair']

print(classification_report(y_test, y_pred_log, target_names=target_names))
The logisctic regression model has an accuracy of 0.67. This is a good start for a baseline model.
### 4.2 Decision Tree
> Decision trees are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features.
# Instantiate the Decision Tree Classifier
clf = DecisionTreeClassifier(criterion='entropy', random_state=39)

# Fit train data to classifier
clf.fit(X_train, y_train)

# Predict the labels of the test set: y_pred
y_pred_dt = clf.predict(X_test)

# Perform cross-validation: cv_results
cv_results_dt = cross_validate(clf, X_test, y_test, cv=3)

# Display cross validation test scores
cv_results_dt['test_score']
# Plot the confusion matrix
fig, ax = plt.subplots(figsize=(10,10))

plot_confusion_matrix(clf, X_test, y_test, cmap='Blues', ax=ax)
plt.title("Decision Tree Confusion Matrix");
# Classification report
target_names = ['Non-Functional', 'Functional', 'Needs-Repair']

print(classification_report(y_test, y_pred_dt, target_names=target_names))
From our classification report, we can see that the accuracy of our model is 0.70. This is a better result that the first model that we used, however, it is still less less than our desired accuracy of 75%
#### 4.3 K Nearest Neighbors
> KNN is a simple algorithm that stores all available cases and classifies new cases based on a similarity measure (e.g., distance functions). KNN has been used in statistical estimation and pattern recognition already in the beginning of 1970â€™s as a non-parametric technique.
# #Build a pipeline with KNN and a scaler
# scaled_knn_pipeline = Pipeline([('ss', StandardScaler()),
#                               ('knn', KNeighborsClassifier())]) 

# # Fit KNN pipeline
# scaled_knn_pipeline.fit(X_train, y_train)

# # Check the score of the KNN pipeline
# print("accuracy score :", scaled_knn_pipeline.score(X_test, y_test))
#### 4.4 Random Forest
> Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees.
# Build a pipeline with StandardScaler and RandomForestClassifier
random_pipeline = Pipeline([('ss', StandardScaler()), 
                            ('RF', RandomForestClassifier(random_state=0))])

# Fit the training data to pipeline 
random_pipeline.fit(X_train, y_train)

# Predict the labels of the test set: y_pred_rf
y_pred_rf = random_pipeline.predict(X_test)

# Perform cross-validation: cv_results_rf
cv_results_rf = cross_validate(random_pipeline, X_test, y_test, cv=3)

# Display cross validation test scores
cv_results_rf['test_score']
# Plot the confusion matrix
fig, ax = plt.subplots(figsize=(10,10))

plot_confusion_matrix(random_pipeline, X_test, y_test, cmap='Blues', ax=ax)
plt.title("Random Forest Confusion Matrix");
# Classification report
target_names = ['Non-Functional', 'Functional', 'Needs-Repair']

print(classification_report(y_test, y_pred_rf, target_names=target_names))
The classification report once again shows a model that has an accuracy of 0.7 percent. This is the same as the decision tree model.
param_grid = {
    'learning_rate': [0.1, 0.2],
    'max_depth': [6],
    'min_child_weight': [1, 2],
    'subsample': [0.5, 0.7],
    'n_estimators': [100],
}
grid_clf = GridSearchCV(clf, param_grid, scoring='accuracy', cv=None, n_jobs=1)
grid_clf.fit(X_train, y_train)

best_parameters = grid_clf.best_params_

print('Grid Search found the following optimal parameters: ')
for param_name in sorted(best_parameters.keys()):
    print('%s: %r' % (param_name, best_parameters[param_name]))

training_preds = grid_clf.predict(X_train)
test_preds = grid_clf.predict(X_test)
training_accuracy = accuracy_score(y_train, training_preds)
test_accuracy = accuracy_score(y_test, test_preds)
grid_clf = GridSearchCV(clf, param_grid, scoring='accuracy', cv=None, n_jobs=1)
grid_clf.fit(X_train, y_train)

best_parameters = grid_clf.best_params_

print('Grid Search found the following optimal parameters: ')
for param_name in sorted(best_parameters.keys()):
    print('%s: %r' % (param_name, best_parameters[param_name]))

training_preds = grid_clf.predict(X_train)
test_preds = grid_clf.predict(X_test)
training_accuracy = accuracy_score(y_train, training_preds)
test_accuracy = accuracy_score(y_test, test_preds)

print('')
print('Training Accuracy: {:.4}%'.format(training_accuracy * 100))
print('Validation accuracy: {:.4}%'.format(test_accuracy * 100))
#### Evaluation
> The evaluation metric for this competition is the categorization accuracy of each well's operational status. This is the percentage of pumps that are classified correctly.

In this project, we have used the following alogrithms to predict the status of a Tanzanian well and these were the results:

##### Logistic Regression
- The logistic regression model returned and accuracy of 67%. Though this was not our desired accuracy, it was a good for a baseline model and gave us a good starting point for our other models.
- The confusion maxtrix evaluation showed that the model had a bias towards predicting that a well is functional. Furthermore, we saw that the model had a higher number of true positives and true negatives than false positives and false negatives. This therefore meant that the model was not overfitting.
- The cross validation results returned consistent scores. This ensured that we were not overfitting our data

##### Decision Tree
- The decision tree model returned an accuracy of 70%. This was a better result that our baseline model (logistic regression). However, despite improving the accuracy, it was still less than our desired accuracy of 75%.
- The cross validation once again returned balanced consistent values accross the tests
- The confusion maxtrix evaluation showed that the model had a bias towards predicting that a well is functional. Furthermore, we saw that the model had a higher number of true positives and true negatives than false positives and false negatives. This therefore meant that the model was not overfitting.

### How did we evaluate our models?
- We used a pipeline to scale the data and then fit it to the model. We then used cross validation to ensure that our model was not overfitting. We also used a confusion matrix to evaluate the performance of our model.
- Despite the fact that our models did not achieve the desired accuracy of 75%, we were able to achieve an accuracy of 70% which is a good start for our project and is within an acceptable range of +/- 5%


